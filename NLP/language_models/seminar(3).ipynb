{"cells":[{"cell_type":"markdown","metadata":{"cellId":"k1gpzj4guo8e1riwj3om1k","id":"NUV7ETqfSgrc"},"source":["### N-gram language models or how to write scientific papers (4 pts)\n","\n","We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n","\n","![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n","\n","_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n","\n","_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"]},{"cell_type":"code","execution_count":1,"metadata":{"cellId":"u8jdaiy68oib3jvr4k01","id":"8TWzRcFTSgre","executionInfo":{"status":"ok","timestamp":1741836040317,"user_tz":-180,"elapsed":913,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"cellId":"0c76vnyl3zui9yhtkodgrlf","colab":{"base_uri":"https://localhost:8080/","height":761},"id":"Of29JE5qSgrf","executionInfo":{"status":"ok","timestamp":1741836045646,"user_tz":-180,"elapsed":5322,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"1d2ed632-2188-498f-87d3-492b4c9e545f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-13 03:20:40--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6016:18::a27d:112\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.dropbox.com/scl/fi/0mulrothty5o8i8ud9gz2/arxivData.json.tar.gz?rlkey=n759u5qx2xpxxglmrl390vwvk&dl=1 [following]\n","--2025-03-13 03:20:40--  https://www.dropbox.com/scl/fi/0mulrothty5o8i8ud9gz2/arxivData.json.tar.gz?rlkey=n759u5qx2xpxxglmrl390vwvk&dl=1\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com/cd/0/inline/Cly6e3VZg_nAE-tTsDc543QRxCTMZMjNa5JuOj8fcs4lY8WHwU1ao2vlRmuR5QD5tRSOpx_8qZ2y_WMM6-C-SE0sFsO67SRD2B4a38WD2PA8311DCf57LnBmdzB1xTPbLK4/file?dl=1# [following]\n","--2025-03-13 03:20:40--  https://uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com/cd/0/inline/Cly6e3VZg_nAE-tTsDc543QRxCTMZMjNa5JuOj8fcs4lY8WHwU1ao2vlRmuR5QD5tRSOpx_8qZ2y_WMM6-C-SE0sFsO67SRD2B4a38WD2PA8311DCf57LnBmdzB1xTPbLK4/file?dl=1\n","Resolving uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com (uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6016:15::a27d:10f\n","Connecting to uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com (uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/ClypWqPV0kKzuRNCqp-RptNKXVpQ3cqkDoNvBD4fXWIdLTuEeqgiDIgQTwNt7fCpxftsD8XbEZzhsDonfMgElK_1uXAtwvvRyOejSJJGEyGmWrSHeIkHWuYHzEBnY_hvfgHNidUMddb4fbEARfL_LwCpnw9grtP7bZg0r4Ws3ZWclAkHuPnBaFVKUMD1ejgf92dGQLABwfAEox3Je3xzyX1tfL2XI5MymtulqBui2FK2IRaCKr5aO0GcYzT2Vn3ES6R3K3bGhacOIZdujRxjv6vIAdf52BYnjRKMHwnchPRIBo8rMJEF8Y192MJ_PdG-I1sEpFFOi9vrKdFDj0VePu5amnRAbrDxmH1ZPsuSgKiqSg/file?dl=1 [following]\n","--2025-03-13 03:20:41--  https://uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com/cd/0/inline2/ClypWqPV0kKzuRNCqp-RptNKXVpQ3cqkDoNvBD4fXWIdLTuEeqgiDIgQTwNt7fCpxftsD8XbEZzhsDonfMgElK_1uXAtwvvRyOejSJJGEyGmWrSHeIkHWuYHzEBnY_hvfgHNidUMddb4fbEARfL_LwCpnw9grtP7bZg0r4Ws3ZWclAkHuPnBaFVKUMD1ejgf92dGQLABwfAEox3Je3xzyX1tfL2XI5MymtulqBui2FK2IRaCKr5aO0GcYzT2Vn3ES6R3K3bGhacOIZdujRxjv6vIAdf52BYnjRKMHwnchPRIBo8rMJEF8Y192MJ_PdG-I1sEpFFOi9vrKdFDj0VePu5amnRAbrDxmH1ZPsuSgKiqSg/file?dl=1\n","Reusing existing connection to uccd705ba987099d180d520e1ba9.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 18933283 (18M) [application/binary]\n","Saving to: ‘arxivData.json.tar.gz’\n","\n","arxivData.json.tar. 100%[===================>]  18.06M  48.1MB/s    in 0.4s    \n","\n","2025-03-13 03:20:42 (48.1 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n","\n","arxivData.json\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                  author  day            id  \\\n","16384  [{'name': 'Zhongqing Wang'}, {'name': 'Yue Zha...    6  1702.01517v1   \n","7229                         [{'name': 'Kazuyuki Hara'}]    9  1711.03343v1   \n","36990                       [{'name': 'Michael Ruster'}]   14  1602.04473v1   \n","33884  [{'name': 'Ryan Pyle'}, {'name': 'Robert Rosen...    8  1803.03304v1   \n","23407  [{'name': 'Zhaowen Wang'}, {'name': 'Jinjun Wa...    9   1203.1985v1   \n","\n","                                                    link  month  \\\n","16384  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n","7229   [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n","36990  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n","33884  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n","23407  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n","\n","                                                 summary  \\\n","16384  We present opinion recommendation, a novel tas...   \n","7229   Deep learning is the state-of-the-art in field...   \n","36990  With the growth of the Semantic Web in size an...   \n","33884  Many recent studies of the motor system are di...   \n","23407  This paper introduces a probabilistic graphica...   \n","\n","                                                     tag  \\\n","16384  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n","7229   [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n","36990  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n","33884  [{'term': 'q-bio.NC', 'scheme': 'http://arxiv....   \n","23407  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n","\n","                                                   title  year  \n","16384   Opinion Recommendation using Neural Memory Model  2017  \n","7229              Analysis of Dropout in Online Learning  2017  \n","36990                     Large-Scale Reasoning with OWL  2016  \n","33884  A model of reward-modulated motor learning wit...  2018  \n","23407  Substructure and Boundary Modeling for Continu...  2012  "],"text/html":["\n","  <div id=\"df-d2dc1964-8e6f-4deb-88bc-0c8a1747ecda\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>day</th>\n","      <th>id</th>\n","      <th>link</th>\n","      <th>month</th>\n","      <th>summary</th>\n","      <th>tag</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>16384</th>\n","      <td>[{'name': 'Zhongqing Wang'}, {'name': 'Yue Zha...</td>\n","      <td>6</td>\n","      <td>1702.01517v1</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>2</td>\n","      <td>We present opinion recommendation, a novel tas...</td>\n","      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n","      <td>Opinion Recommendation using Neural Memory Model</td>\n","      <td>2017</td>\n","    </tr>\n","    <tr>\n","      <th>7229</th>\n","      <td>[{'name': 'Kazuyuki Hara'}]</td>\n","      <td>9</td>\n","      <td>1711.03343v1</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>11</td>\n","      <td>Deep learning is the state-of-the-art in field...</td>\n","      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n","      <td>Analysis of Dropout in Online Learning</td>\n","      <td>2017</td>\n","    </tr>\n","    <tr>\n","      <th>36990</th>\n","      <td>[{'name': 'Michael Ruster'}]</td>\n","      <td>14</td>\n","      <td>1602.04473v1</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>2</td>\n","      <td>With the growth of the Semantic Web in size an...</td>\n","      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n","      <td>Large-Scale Reasoning with OWL</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>33884</th>\n","      <td>[{'name': 'Ryan Pyle'}, {'name': 'Robert Rosen...</td>\n","      <td>8</td>\n","      <td>1803.03304v1</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>3</td>\n","      <td>Many recent studies of the motor system are di...</td>\n","      <td>[{'term': 'q-bio.NC', 'scheme': 'http://arxiv....</td>\n","      <td>A model of reward-modulated motor learning wit...</td>\n","      <td>2018</td>\n","    </tr>\n","    <tr>\n","      <th>23407</th>\n","      <td>[{'name': 'Zhaowen Wang'}, {'name': 'Jinjun Wa...</td>\n","      <td>9</td>\n","      <td>1203.1985v1</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>3</td>\n","      <td>This paper introduces a probabilistic graphica...</td>\n","      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n","      <td>Substructure and Boundary Modeling for Continu...</td>\n","      <td>2012</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2dc1964-8e6f-4deb-88bc-0c8a1747ecda')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d2dc1964-8e6f-4deb-88bc-0c8a1747ecda button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d2dc1964-8e6f-4deb-88bc-0c8a1747ecda');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cb5b0fe2-a5f4-4f6d-b5b3-859b71843816\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb5b0fe2-a5f4-4f6d-b5b3-859b71843816')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cb5b0fe2-a5f4-4f6d-b5b3-859b71843816 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'name': 'Kazuyuki Hara'}]\",\n          \"[{'name': 'Zhaowen Wang'}, {'name': 'Jinjun Wang'}, {'name': 'Jing Xiao'}, {'name': 'Kai-Hsiang Lin'}, {'name': 'Thomas Huang'}]\",\n          \"[{'name': 'Michael Ruster'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 6,\n        \"max\": 14,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          9,\n          8,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1711.03343v1\",\n          \"1203.1985v1\",\n          \"1602.04473v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1711.03343v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1711.03343v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1203.1985v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1203.1985v1', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1602.04473v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1602.04473v1', 'type': 'application/pdf', 'title': 'pdf'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 11,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          11,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Deep learning is the state-of-the-art in fields such as visual object\\nrecognition and speech recognition. This learning uses a large number of layers\\nand a huge number of units and connections. Therefore, overfitting is a serious\\nproblem with it, and the dropout which is a kind of regularization tool is\\nused. However, in online learning, the effect of dropout is not well known.\\nThis paper presents our investigation on the effect of dropout in online\\nlearning. We analyzed the effect of dropout on convergence speed near the\\nsingular point. Our results indicated that dropout is effective in online\\nlearning. Dropout tends to avoid the singular point for convergence speed near\\nthat point.\",\n          \"This paper introduces a probabilistic graphical model for continuous action\\nrecognition with two novel components: substructure transition model and\\ndiscriminative boundary model. The first component encodes the sparse and\\nglobal temporal transition prior between action primitives in state-space model\\nto handle the large spatial-temporal variations within an action class. The\\nsecond component enforces the action duration constraint in a discriminative\\nway to locate the transition boundaries between actions more accurately. The\\ntwo components are integrated into a unified graphical structure to enable\\neffective training and inference. Our comprehensive experimental results on\\nboth public and in-house datasets show that, with the capability to incorporate\\nadditional information that had not been explicitly or efficiently modeled by\\nprevious methods, our proposed algorithm achieved significantly improved\\nperformance for continuous action recognition.\",\n          \"With the growth of the Semantic Web in size and importance, more and more\\nknowledge is stored in machine-readable formats such as the Web Ontology\\nLanguage OWL. This paper outlines common approaches for efficient reasoning on\\nlarge-scale data consisting of billions ($10^9$) of triples. Therefore, OWL and\\nits sublanguages, as well as forward and backward chaining techniques are\\npresented. The WebPIE reasoner is discussed in detail as an example for forward\\nchaining using MapReduce for materialisation. Moreover, the QueryPIE reasoner\\nis presented as a backward chaining/hybrid approach which uses query rewriting.\\nFurthermore, an overview on other reasoners is given such as OWLIM and TrOWL.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DB', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Analysis of Dropout in Online Learning\",\n          \"Substructure and Boundary Modeling for Continuous Action Recognition\",\n          \"Large-Scale Reasoning with OWL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2012,\n        \"max\": 2018,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2016,\n          2012,\n          2017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}],"source":["# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n","!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n","!tar -xvzf arxivData.json.tar.gz\n","data = pd.read_json(\"./arxivData.json\")\n","data.sample(n=5)"]},{"cell_type":"code","source":["data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2JSOtGlTPH8","executionInfo":{"status":"ok","timestamp":1741836045661,"user_tz":-180,"elapsed":11,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"c44cd00e-da0c-4218-8825-762ccca4d0e4"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(41000, 9)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["data['summary'].iloc[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"3Sf4iO-_TRia","executionInfo":{"status":"ok","timestamp":1741836045729,"user_tz":-180,"elapsed":66,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"092bd18b-707e-42d3-d700-a820bc52e547"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"metadata":{"cellId":"lbyqb5rx7j8jpo591r06ak","colab":{"base_uri":"https://localhost:8080/"},"id":"8DWZSYeeSgrg","executionInfo":{"status":"ok","timestamp":1741836046286,"user_tz":-180,"elapsed":550,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"2e08601e-82f5-4582-f2e9-c392392d967f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Differential Contrastive Divergence ; This paper has been retracted.',\n"," 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n"," 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"]},"metadata":{},"execution_count":5}],"source":["# assemble lines: concatenate title and description\n","lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n","\n","sorted(lines, key=len)[:3]"]},{"cell_type":"markdown","metadata":{"cellId":"7u97m5s8ekl5zd5a43a1yc","id":"Tr8SivX5Sgrg"},"source":["### Tokenization\n","\n","You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"cellId":"u8rvfk719iek97t3rarwr","id":"wHBkStWvSgrh","executionInfo":{"status":"ok","timestamp":1741836054877,"user_tz":-180,"elapsed":8582,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["# Task: convert lines (in-place) into strings of space-separated tokens. Import & use WordPunctTokenizer\n","import nltk\n","from nltk.tokenize import WordPunctTokenizer\n","tokenizer = WordPunctTokenizer()\n","\n","lines = [\" \".join(tokenizer.tokenize(line.lower())) for line in lines]"]},{"cell_type":"code","source":["lines[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bi-U9djiUV5c","executionInfo":{"status":"ok","timestamp":1741836054902,"user_tz":-180,"elapsed":5,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"f51ae709-4779-4d74-e42f-9fe2fda7ae44"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['dual recurrent attention units for visual question answering ; we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention . the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question . our single model outperforms the first place winner on the vqa 1 . 0 dataset , performs within margin to the current state - of - the - art ensemble model . we also experiment with replacing attention mechanisms in other state - of - the - art models with our implementation and show increased accuracy . in both cases , our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset .',\n"," 'sequential short - text classification with recurrent and convolutional neural networks ; recent approaches based on artificial neural networks ( anns ) have shown promising results for short - text classification . however , many short texts occur in sequences ( e . g ., sentences in a document or utterances in a dialog ), and most existing ann - based systems do not leverage the preceding short texts when classifying a subsequent one . in this work , we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts . our model achieves state - of - the - art results on three different datasets for dialog act prediction .',\n"," 'multiresolution recurrent neural networks : an application to dialogue response generation ; we introduce the multiresolution recurrent neural network , which extends the sequence - to - sequence framework to model natural language generation as two parallel discrete stochastic processes : a sequence of high - level coarse tokens , and a sequence of natural language tokens . there are many ways to estimate or learn the high - level coarse tokens , but we argue that a simple extraction procedure is sufficient to capture a wealth of high - level discourse semantics . such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log - likelihood over both sequences . in contrast to the standard log - likelihood objective w . r . t . natural language tokens ( word perplexity ), optimizing the joint log - likelihood biases the model towards modeling high - level abstractions . we apply the proposed model to the task of dialogue response generation in two challenging domains : the ubuntu technical support domain , and twitter conversations . on ubuntu , the model outperforms competing approaches by a substantial margin , achieving state - of - the - art results according to both automatic evaluation metrics and a human evaluation study . on twitter , the model appears to generate more relevant and on - topic responses according to automatic evaluation metrics . finally , our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long - term structure .']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"metadata":{"cellId":"w88nddpp2k8edoeyyyjh0l","id":"XzKtU8p7Sgri","executionInfo":{"status":"ok","timestamp":1741836054967,"user_tz":-180,"elapsed":64,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["assert sorted(lines, key=len)[0] == \\\n","    'differential contrastive divergence ; this paper has been retracted .'\n","assert sorted(lines, key=len)[2] == \\\n","    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"]},{"cell_type":"markdown","metadata":{"cellId":"qb6h3hxmr095egzv8rlzul","id":"X4MnX9JtSgri"},"source":["### N-Gram Language Model (1point)\n","\n","A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n","\n","It can do so by following the chain rule:\n","$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$\n","\n","The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n","\n","One popular approximation is to assume that next word only depends on a finite amount of previous words:\n","\n","$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n","\n","Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words.\n","\n","$$\n","    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n","$$\n","\n","You can also sometimes see such approximation under the name of _n-th order markov assumption_."]},{"cell_type":"markdown","metadata":{"cellId":"u68wydbiioqlp5gl96mhd","id":"bV4Jbn4OSgrj"},"source":["The first stage to building such a model is counting all word occurences given N-1 previous words"]},{"cell_type":"code","source":["tuple(range(19))[3:6]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJis7xReaAd1","executionInfo":{"status":"ok","timestamp":1741836071638,"user_tz":-180,"elapsed":51,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"81963755-2960-47cb-e2ae-56df7d2f0d00"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 4, 5)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":15,"metadata":{"cellId":"og84gjipnumsakhiiu9ap","id":"Dry-4yPZSgrk","executionInfo":{"status":"ok","timestamp":1741836071711,"user_tz":-180,"elapsed":11,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["from tqdm import tqdm\n","from collections import defaultdict, Counter\n","\n","# special tokens:\n","# - `UNK` represents absent tokens,\n","# - `EOS` is a special token after the end of sequence\n","\n","UNK, EOS = \"_UNK_\", \"_EOS_\"\n","\n","def count_ngrams(lines, n):\n","    \"\"\"\n","    Count how many times each word occured after (n - 1) previous words\n","    :param lines: an iterable of strings with space-separated tokens\n","    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n","\n","    When building counts, please consider the following two edge cases:\n","    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n","      empty prefix: \"\" -> (UNK, UNK)\n","      short prefix: \"the\" -> (UNK, the)\n","      long prefix: \"the new approach\" -> (new, approach)\n","    - you should add a special token, EOS, at the end of each sequence\n","      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n","      count the probability of this token just like all others.\n","    \"\"\"\n","    counts = defaultdict(Counter)\n","    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n","    for line in lines:\n","        line = tuple((line + ' ' + EOS).split())\n","        for idx, tok in enumerate(line):\n","            prefix = line[max(idx-n+1, 0):idx]\n","            prefix = (UNK,) * (n - 1 - len(prefix)) + prefix\n","            counts[prefix][tok] += 1\n","\n","    return counts\n"]},{"cell_type":"code","execution_count":16,"metadata":{"cellId":"xyf2he6lak9mmqarl3nck","id":"7d3cuNYMSgrl","executionInfo":{"status":"ok","timestamp":1741836073008,"user_tz":-180,"elapsed":25,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["# let's test it\n","dummy_lines = sorted(lines, key=len)[:100]\n","dummy_counts = count_ngrams(dummy_lines, n=3)\n","assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n","assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n","assert dummy_counts['_UNK_', 'a']['note'] == 3\n","assert dummy_counts['p', '=']['np'] == 2\n","assert dummy_counts['author', '.']['_EOS_'] == 1"]},{"cell_type":"markdown","metadata":{"cellId":"4j620npeqvj0k8ak8xqx8xk","id":"TelqYy3MSgrl"},"source":["Once we can count N-grams, we can build a probabilistic language model.\n","The simplest way to compute probabilities is in proporiton to counts:\n","\n","$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"]},{"cell_type":"code","source":["sum(dummy_counts[(\"of\", \"the\")].values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m0_DqSF4bey1","executionInfo":{"status":"ok","timestamp":1741836074516,"user_tz":-180,"elapsed":14,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"ff54b359-ba49-435e-b014-7865d6da087a"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["23"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":18,"metadata":{"cellId":"c7cm76wmzlaa12bctznzei","id":"vOBHt9XFSgrm","executionInfo":{"status":"ok","timestamp":1741836075931,"user_tz":-180,"elapsed":23,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["class NGramLanguageModel:\n","    def __init__(self, lines, n):\n","        \"\"\"\n","        Train a simple count-based language model:\n","        compute probabilities P(w_t | prefix) given ngram counts\n","\n","        :param n: computes probability of next token given (n - 1) previous words\n","        :param lines: an iterable of strings with space-separated tokens\n","        \"\"\"\n","        assert n >= 1\n","        self.n = n\n","\n","        counts = count_ngrams(lines, self.n)\n","\n","        # compute token proabilities given counts\n","        self.probs = defaultdict(Counter)\n","        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n","        for prefix in counts:\n","            sm = sum(counts[prefix].values())\n","            for tok in counts[prefix]:\n","                self.probs[prefix][tok] = counts[prefix][tok] / sm\n","\n","        # populate self.probs with actual probabilities\n","\n","    def get_possible_next_tokens(self, prefix):\n","        \"\"\"\n","        :param prefix: string with space-separated prefix tokens\n","        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n","        \"\"\"\n","        prefix = prefix.split()\n","        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n","        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n","        return self.probs[tuple(prefix)]\n","\n","    def get_next_token_prob(self, prefix, next_token):\n","        \"\"\"\n","        :param prefix: string with space-separated prefix tokens\n","        :param next_token: the next token to predict probability for\n","        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n","        \"\"\"\n","        return self.get_possible_next_tokens(prefix).get(next_token, 0)"]},{"cell_type":"markdown","metadata":{"cellId":"0ftnn4nmuzrup6c0vvhb8q","id":"Q1vrRvhvSgrm"},"source":["Let's test it!"]},{"cell_type":"code","execution_count":19,"metadata":{"cellId":"a7zajcnvhqupvcrmacvkur","id":"4rPh7rK0Sgrn","executionInfo":{"status":"ok","timestamp":1741836076981,"user_tz":-180,"elapsed":14,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n","\n","p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n","assert np.allclose(p_initial['learning'], 0.02)\n","assert np.allclose(p_initial['a'], 0.13)\n","assert np.allclose(p_initial.get('meow', 0), 0)\n","assert np.allclose(sum(p_initial.values()), 1)\n","\n","p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n","assert np.allclose(p_a['machine'], 0.15384615)\n","assert np.allclose(p_a['note'], 0.23076923)\n","assert np.allclose(p_a.get('the', 0), 0)\n","assert np.allclose(sum(p_a.values()), 1)\n","\n","assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n","assert dummy_lm.get_possible_next_tokens('a machine') == \\\n","    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n","    \"your 3-gram model should only depend on 2 previous words\""]},{"cell_type":"markdown","metadata":{"cellId":"oh8r9a41kuk4r51wra9","id":"SGhVNKiWSgrn"},"source":["Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."]},{"cell_type":"code","execution_count":20,"metadata":{"cellId":"f17xoejjppmooo2nopw4xo","id":"ko71iudjSgrn","executionInfo":{"status":"ok","timestamp":1741836103834,"user_tz":-180,"elapsed":26355,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["lm = NGramLanguageModel(lines, n=3)"]},{"cell_type":"markdown","metadata":{"cellId":"2kd9glwnkr470qc4bt7f1e","id":"Kjg3rL1USgro"},"source":["The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n","\n","$ X = [] $\n","\n","__forever:__\n","* $w_{next} \\sim P(w_{next} | X)$\n","* $X = concat(X, w_{next})$\n","\n","\n","Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n","\n","$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n","\n","Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."]},{"cell_type":"code","source":["np.array(range(10)) ** 1/2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DPqXaRbf3A1","executionInfo":{"status":"ok","timestamp":1741836482923,"user_tz":-180,"elapsed":14,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"6423389c-3050-4e1b-dccc-705f4765397e"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["np.random.choice([1, 2, 3], size=1, p=[3, 2, 5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"fhsioKgKgWe5","executionInfo":{"status":"error","timestamp":1741836198689,"user_tz":-180,"elapsed":14,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"708351d2-1d39-413a-846b-ec9d806dc952"},"execution_count":22,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"probabilities do not sum to 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-b37f86435474>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"]}]},{"cell_type":"code","execution_count":43,"metadata":{"cellId":"sgbatlm9vzb4z889fho7","id":"3qIAvHO-Sgro","executionInfo":{"status":"ok","timestamp":1741836558736,"user_tz":-180,"elapsed":12,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["def get_next_token(lm, prefix, temperature=1.0):\n","    \"\"\"\n","    return next token after prefix;\n","    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n","        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n","    \"\"\"\n","    tokens, probs = zip(*lm.get_possible_next_tokens(prefix).items())\n","    tokens, probs = np.array(tokens), np.array(probs)\n","    if temperature == 0:\n","        return tokens[probs.argmax()]\n","    probs = probs ** (1/temperature)\n","    # print(*zip(tokens, probs))\n","    return np.random.choice(tokens, p=probs/probs.sum())\n"]},{"cell_type":"code","source":["get_next_token(lm, 'deep', temperature=0.5)\n","np.arange(10).argmax()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D47ZPrV3iHZr","executionInfo":{"status":"ok","timestamp":1741836560513,"user_tz":-180,"elapsed":6,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"4a3e9804-7581-43a0-9d92-0875c8d0ec52"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","execution_count":45,"metadata":{"cellId":"98l40131wjtd5xbdm5b2nr","colab":{"base_uri":"https://localhost:8080/"},"id":"0QG5MbZASgrp","executionInfo":{"status":"ok","timestamp":1741836566510,"user_tz":-180,"elapsed":5854,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"f597db17-92ae-4186-c1cf-412b446ac72d"},"outputs":[{"output_type":"stream","name":"stdout","text":["8580\n","Looks nice!\n"]}],"source":["from collections import Counter\n","test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n","assert 250 < test_freqs['not'] < 450\n","assert 8500 < test_freqs['been'] < 9500\n","assert 1 < test_freqs['lately'] < 200\n","\n","test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n","assert 1500 < test_freqs['learning'] < 3000\n","test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n","# print(test_freqs['learning'])\n","assert 8000 < test_freqs['learning'] < 9000\n","test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n","assert test_freqs['learning'] == 10000\n","\n","print(\"Looks nice!\")"]},{"cell_type":"markdown","metadata":{"cellId":"ux4n8iq523n4s3ftrelhxj","id":"jtz4VuE7Sgrp"},"source":["Let's have fun with this model"]},{"cell_type":"code","execution_count":46,"metadata":{"cellId":"1nnnycga61rijt6nd8zai","colab":{"base_uri":"https://localhost:8080/"},"id":"WB9v1LjXSgrp","executionInfo":{"status":"ok","timestamp":1741837149744,"user_tz":-180,"elapsed":56,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"0f1220fb-32cf-4285-8f6e-85db58a1e475"},"outputs":[{"output_type":"stream","name":"stdout","text":["artificial intelligence ; this paper , we propose a novel ppgi system was developed and can be used to extract geometric features in data of a cyclist and the environment seen during training . _EOS_\n"]}],"source":["prefix = 'artificial' # <- your ideas :)\n","\n","for i in range(100):\n","    prefix += ' ' + get_next_token(lm, prefix)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","\n","print(prefix)"]},{"cell_type":"code","execution_count":47,"metadata":{"cellId":"pxyjsv3b7r8thdfxlgitl","colab":{"base_uri":"https://localhost:8080/"},"id":"72zUfKETSgrp","executionInfo":{"status":"ok","timestamp":1741837170275,"user_tz":-180,"elapsed":16,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"dc036e96-264a-43df-bdf7-861f1db0b615"},"outputs":[{"output_type":"stream","name":"stdout","text":["bridging the gap between the observed data . we propose a new type of function evaluations compared to the new domain - specific word representation ; the paper . _EOS_\n"]}],"source":["prefix = 'bridging the' # <- more of your ideas\n","\n","for i in range(100):\n","    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","\n","print(prefix)"]},{"cell_type":"markdown","metadata":{"cellId":"2n90bscmzfko0qnctp7ysc","id":"RYOn24BfSgrq"},"source":["__More in the homework:__ nucleus sampling, top-k sampling, beam search(not for the faint of heart)."]},{"cell_type":"markdown","metadata":{"cellId":"3gdmey7g8at5n5c5x4gayh","id":"h0D0zDCjSgrq"},"source":["### Evaluating language models: perplexity (1point)\n","\n","Perplexity is a measure of how well your model approximates the true probability distribution behind the data. __Smaller perplexity = better model__.\n","\n","To compute perplexity on one sentence, use:\n","$$\n","    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n","$$\n","\n","\n","On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of $1/N$, where $N$ is __total length (in tokens) of all sentences__ in corpora.\n","\n","This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."]},{"cell_type":"code","source":["np.log(0) < 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sd-JY6p2ocx7","executionInfo":{"status":"ok","timestamp":1741837958663,"user_tz":-180,"elapsed":22,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"1ff5b488-1eee-463d-979b-e3ed83c0433d"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-49-6f86d14b64be>:1: RuntimeWarning: divide by zero encountered in log\n","  np.log(0) < 10\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","execution_count":81,"metadata":{"cellId":"5hp010xyzzb4vqewo1bhny","id":"X0JMrQFqSgrq","executionInfo":{"status":"ok","timestamp":1741838936035,"user_tz":-180,"elapsed":17,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n","    \"\"\"\n","    :param lines: a list of strings with space-separated tokens\n","    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n","    :returns: corpora-level perplexity - a single scalar number from the formula above\n","\n","    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n","\n","    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n","    \"\"\"\n","    perplexity = 0\n","    total_length = 0\n","    for line in lines:\n","        line = tuple((line + ' ' + EOS).split())\n","        total_length += len(line)\n","        prefix = [UNK] * (lm.n-1)\n","        for tok in line:\n","            prob = lm.get_next_token_prob(\" \".join(prefix), tok)\n","            perplexity += max(min_logprob, np.log(prob))\n","            prefix = prefix[1:] + [tok]\n","\n","    # return np.exp(perplexity)**(-1/total_length)\n","    return np.exp(-perplexity / total_length)"]},{"cell_type":"code","source":["(np.exp(3))**(1/2), np.exp(3/2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g13fouFlrRt9","executionInfo":{"status":"ok","timestamp":1741838936211,"user_tz":-180,"elapsed":30,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"e9ad7fe6-551e-449d-fe7d-f54c38eb60ad"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4.4816890703380645, 4.4816890703380645)"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","execution_count":83,"metadata":{"cellId":"8b689bobhkey04x7pabupj","colab":{"base_uri":"https://localhost:8080/"},"id":"rXApGkWMSgrr","executionInfo":{"status":"ok","timestamp":1741838937786,"user_tz":-180,"elapsed":85,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"74789a8c-fc54-409b-8c0e-435ce27166c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n","318.2132342216302 1.5199996213739575 1.1838145037901249\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-81-b5f3c003645d>:19: RuntimeWarning: divide by zero encountered in log\n","  perplexity += max(min_logprob, np.log(prob))\n"]}],"source":["lm1 = NGramLanguageModel(dummy_lines, n=1)\n","lm3 = NGramLanguageModel(dummy_lines, n=3)\n","lm10 = NGramLanguageModel(dummy_lines, n=10)\n","\n","ppx1 = perplexity(lm1, dummy_lines)\n","ppx3 = perplexity(lm3, dummy_lines)\n","ppx10 = perplexity(lm10, dummy_lines)\n","ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n","\n","print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n","\n","print(ppx1, ppx3, ppx10)\n","assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be non-negative and reasonably small\"\n","assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n","assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n","    \" Make sure you use min_logprob right\"\n","assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"]},{"cell_type":"markdown","metadata":{"cellId":"ypc4lks4vs1li908fqi8","id":"aBSjVRgsSgrr"},"source":["Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."]},{"cell_type":"code","execution_count":84,"metadata":{"cellId":"tjnehsem2lmijkg2lto4w","colab":{"base_uri":"https://localhost:8080/"},"id":"vWpWetgHSgrr","executionInfo":{"status":"ok","timestamp":1741839995879,"user_tz":-180,"elapsed":60153,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"eab74965-ccb8-4e29-e636-010fca83d37d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-81-b5f3c003645d>:19: RuntimeWarning: divide by zero encountered in log\n","  perplexity += max(min_logprob, np.log(prob))\n"]},{"output_type":"stream","name":"stdout","text":["N = 1, Perplexity = 1832.23136\n","N = 2, Perplexity = 85653987.28774\n","N = 3, Perplexity = 61999196259043346743296.00000\n"]}],"source":["from sklearn.model_selection import train_test_split\n","train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n","\n","for n in (1, 2, 3):\n","    lm = NGramLanguageModel(n=n, lines=train_lines)\n","    ppx = perplexity(lm, test_lines)\n","    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellId":"38nfbfkpzgfxik8kccyt1l","id":"rVRROSnSSgrr"},"outputs":[],"source":["# whoops, it just blew up :)"]},{"cell_type":"markdown","metadata":{"cellId":"oopn2o57wxm9vbxzycytce","id":"w3AoKePySgrs"},"source":["### LM Smoothing\n","\n","The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n","\n","To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n","\n","$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n","\n","If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n","\n","Here's an example code we've implemented for you:"]},{"cell_type":"code","execution_count":87,"metadata":{"cellId":"ioh26rlov6g8l2ssj1c8pm","id":"kT6LUqO-Sgrs","executionInfo":{"status":"ok","timestamp":1741842318921,"user_tz":-180,"elapsed":159,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["class LaplaceLanguageModel(NGramLanguageModel):\n","    \"\"\" this code is an example, no need to change anything \"\"\"\n","    def __init__(self, lines, n, delta=1.0):\n","        self.n = n\n","        counts = count_ngrams(lines, self.n)\n","        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n","        self.probs = defaultdict(Counter)\n","\n","        for prefix in counts:\n","            token_counts = counts[prefix]\n","            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n","            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n","                                          for token in token_counts}\n","    def get_possible_next_tokens(self, prefix):\n","        token_probs = super().get_possible_next_tokens(prefix)\n","        missing_prob_total = 1.0 - sum(token_probs.values())\n","        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n","        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n","\n","    def get_next_token_prob(self, prefix, next_token):\n","        token_probs = super().get_possible_next_tokens(prefix)\n","        if next_token in token_probs:\n","            return token_probs[next_token]\n","        else:\n","            missing_prob_total = 1.0 - sum(token_probs.values())\n","            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n","            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",""]},{"cell_type":"markdown","metadata":{"cellId":"90vsann3920ie05r2blbmi","execution_id":"3868303d-0bb9-42c6-a9a8-dcf485c8220c","id":"PXZ498JySgrs"},"source":["**Disclaimer**: the implementation above assumes all words unknown within a given context to be equally likely, *as well as the words outside of vocabulary*. Therefore, its' perplexity will be lower than it should when encountering such words. Therefore, comparing it with a model with fewer unknown words will not be fair. When implementing your own smoothing, you may handle this by adding a virtual `UNK` token of non-zero probability. Technically, this will result in a model where probabilities do not add up to $1$, but it is close enough for a practice excercise."]},{"cell_type":"code","execution_count":88,"metadata":{"cellId":"3xvxkdxcmfqucruyt66mdc","id":"F6NnE9OQSgrs","executionInfo":{"status":"ok","timestamp":1741842319456,"user_tz":-180,"elapsed":119,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["#test that it's a valid probability model\n","for n in (1, 2, 3):\n","    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n","    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""]},{"cell_type":"code","source":["len(lm.vocab)"],"metadata":{"id":"YfH7gpGR5fZs","executionInfo":{"status":"ok","timestamp":1741842428612,"user_tz":-180,"elapsed":23,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"59134150-0344-40f8-d59c-ada3003ac1e3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54176"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","execution_count":89,"metadata":{"cellId":"j6zqa50koitjjri9ipd8ec","colab":{"base_uri":"https://localhost:8080/"},"id":"h5AgzGEiSgrt","executionInfo":{"status":"ok","timestamp":1741842390392,"user_tz":-180,"elapsed":70793,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}},"outputId":"0871e8d1-11b5-49ee-cb6e-b24858d6ac04"},"outputs":[{"output_type":"stream","name":"stdout","text":["N = 1, Perplexity = 977.67559\n","N = 2, Perplexity = 470.48021\n","N = 3, Perplexity = 3679.44765\n"]}],"source":["for n in (1, 2, 3):\n","    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n","    ppx = perplexity(lm, test_lines)\n","    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"]},{"cell_type":"code","execution_count":90,"metadata":{"cellId":"pjuqt30jcerwbz1ym9zv1","id":"b77NbAwESgrt","executionInfo":{"status":"ok","timestamp":1741842390402,"user_tz":-180,"elapsed":12,"user":{"displayName":"Лёша Хархалёв","userId":"10683403610633461368"}}},"outputs":[],"source":["# optional: try to sample tokens from such a model"]},{"cell_type":"markdown","metadata":{"cellId":"3b8s1y9uls4fosu3yp28gg","id":"F8fmVItoSgrt"},"source":["### Kneser-Ney smoothing (2 points)\n","\n","Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n","\n","\n","Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n","\n","It can be computed recurrently, for n>1:\n","\n","$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n","\n","where\n","- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n","- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n","- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n","- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n","\n","See lecture slides or wiki for more detailed formulae.\n","\n","__Your task__ is to\n","- implement `KneserNeyLanguageModel` class,\n","- test it on 1-3 gram language models\n","- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"]},{"cell_type":"code","execution_count":null,"metadata":{"cellId":"2ix7kzw02v30oye55322all","id":"pZ5WlLYrSgrt"},"outputs":[],"source":["class KneserNeyLanguageModel(NGramLanguageModel):\n","    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n","    def __init__(self, lines, n, delta=1.0):\n","        self.n = n\n","        <YOUR CODE>\n","\n","    def get_possible_next_tokens(self, prefix):\n","        < YOUR CODE >\n","\n","    def get_next_token_prob(self, prefix, next_token):\n","        <YOUR CODE>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellId":"lsk91832qbmdt7x1q0a8z4","id":"_e6bEnChSgru"},"outputs":[],"source":["#test that it's a valid probability model\n","for n in (1, 2, 3):\n","    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n","    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""]},{"cell_type":"code","execution_count":null,"metadata":{"cellId":"pp3jtkk9annp1qkou58x1b","id":"aiiLB0WzSgru"},"outputs":[],"source":["for n in (1, 2, 3):\n","    lm = KneserNeyLanguageModel(train_lines, n=n, smoothing=<...>)\n","    ppx = perplexity(lm, test_lines)\n","    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"notebookId":"53997d2d-afb8-4477-8874-b6d46299f06c","notebookPath":"seminar.ipynb","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}